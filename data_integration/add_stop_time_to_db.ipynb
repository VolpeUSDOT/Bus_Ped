{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Add stop times\n",
    "\n",
    "Modified from `add_stop_time_to_db.py`\n",
    "\n",
    "Follows Step 2, `add_vehicle_assignments_to_db.ipynb`\n",
    "Calls Step 4, `add_route_stops_to_db.py`  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from os import path, walk\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from add_route_stops_to_db import read_route_stop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script creates or replaces a table in the database at the supplied\n",
    "# path that contains the set of stops for each of five Downtown DASH routes\n",
    "\n",
    "def find_duplicates(df, index_col='stop_time_id', duplicate_col='route_id'):\n",
    "  unique_route_ids = df.loc[:, duplicate_col].unique()\n",
    "\n",
    "  for unique_route_id in unique_route_ids:\n",
    "    routes = df.loc[df[duplicate_col] == unique_route_id]\n",
    "\n",
    "    # display unique record count and total record count for comparison\n",
    "    print(routes.shape[0])\n",
    "    print(routes.loc[:, index_col].unique().shape[0])\n",
    "\n",
    "def preprocess_bus_number(elem):\n",
    "  return elem.split()[-1]\n",
    "\n",
    "# TODO: convert print statements to log statements\n",
    "def read_stop_time_data(data_root_dir):\n",
    "  stop_time_data = []\n",
    "\n",
    "  for dir, subdirs, files in walk(data_root_dir):\n",
    "    # we assume that files only exist at the nodes\n",
    "    if len(files) > 0:\n",
    "      # we assume that only one driver schedule file exists in the current dir\n",
    "      file_name_indices = [\n",
    "        file.find('_StopTimes_') >= 0 for file in files]\n",
    "\n",
    "      try:\n",
    "        file_name_index = file_name_indices.index(True)\n",
    "\n",
    "        file_name = files[file_name_index]\n",
    "\n",
    "        file_path = path.join(dir, file_name)\n",
    "\n",
    "        df = pd.read_csv(\n",
    "          file_path, sep='\\t', usecols=[0, 1, 2, 4, 5, 6, 7, 8, 9, 12],\n",
    "          parse_dates=['arrived_at', 'departed_at'], dtype={\n",
    "            'stop_id': object, 'route_id': np.uint32, 'vehicle_id': np.uint16,\n",
    "            'arrived_at': object, 'arrival_latitude': np.float64,\n",
    "            'arrival_longitude': np.float64, 'departed_at': object,\n",
    "            'departure_latitude': np.float64, 'departure_longitude': np.float64,\n",
    "            'stop_time_id': np.uint64})\n",
    "\n",
    "        # convert null stop_ids to a zero value\n",
    "        df['stop_id'] = np.array(\n",
    "          df['stop_id'].values, dtype=np.float32).astype(np.uint32)\n",
    "\n",
    "        # print(df.head(2))\n",
    "        # print(df.dtypes)\n",
    "\n",
    "        stop_time_data.append(df)\n",
    "      #TODO: discover and handle distinct exceptions rather than catch all\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    stop_time_data = pd.concat(\n",
    "    stop_time_data, ignore_index=True, verify_integrity=True)\n",
    "\n",
    "  # count the unique stop_tim_id and compare with the number of records to\n",
    "  # identify duplicates (and do it per route in case duplicates occur across\n",
    "  # routes but not within a single route - which is okay) we learn that indeed\n",
    "  # the stop ids are unique within a given route\n",
    "  # ...we don't call this anymore having observed that no duplicates exist (for now)\n",
    "  # find_duplicates(stop_time_data)\n",
    "\n",
    "  # drop duplicates if found\n",
    "  stop_time_data.drop_duplicates(inplace=True)\n",
    "\n",
    "  # we temporarily also drop records with missing values to prove our concept.\n",
    "  # Key attributes that require values include 1) __, 2) route_id,\n",
    "  # 3) vehicle_id, 4) arrived_at, 5) departed_at, and 6) stop_time_id. For now,\n",
    "  # we exclude the stop_id because many relevant records have missing stop_ids.\n",
    "  # TODO: Infer missing values where possible using warning and route data\n",
    "  key_column_names = ['route_id', 'vehicle_id', 'arrived_at', 'departed_at']\n",
    "\n",
    "  stop_time_data.dropna(subset=key_column_names, inplace=True)\n",
    "\n",
    "  stop_time_data.drop(stop_time_data.query('stop_id == 0').index, inplace=True)\n",
    "\n",
    "  # we make no assumption about the order in which source files are input\n",
    "  stop_time_data.sort_values(\n",
    "    ['route_id', 'vehicle_id', 'arrived_at', 'departed_at'], inplace=True)\n",
    "\n",
    "  # reset indices after removing some records\n",
    "  stop_time_data.set_index(pd.RangeIndex(stop_time_data.shape[0]), inplace=True)\n",
    "  print(stop_time_data.describe())\n",
    "  return stop_time_data\n",
    "# we must identify terminal stop records and collapse sequences of records of a\n",
    "# single terminal into a single record. We extract the set of terminal stops\n",
    "# from the 'route_stop' table in the existing database\n",
    "\n",
    "# get terminal stops from Excel in case a route stop table has not yet been\n",
    "# created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_stop_time_data(stop_time_data, route_stop_data):\n",
    "  terminal_stop_data = route_stop_data.loc[\n",
    "    route_stop_data.loc[:, 'sequence'] == 1]\n",
    "\n",
    "  terminal_stop_time_data = []\n",
    "\n",
    "  # TODO handle discontinuity at 12AM.\n",
    "  # do any records have timestamps between 2130 and 0030?\n",
    "  for stop_id in terminal_stop_data['stop_id']:\n",
    "    terminal_stop_time_data.append(\n",
    "      stop_time_data[stop_time_data['stop_id'] == stop_id])\n",
    "\n",
    "  terminal_stop_time_data = pd.concat(terminal_stop_time_data)\n",
    "\n",
    "  print('terminal_stop_time_data:\\n{}'.format(terminal_stop_time_data.describe()))\n",
    "\n",
    "  print('stop_time_data pre-drop:\\n{}'.format(stop_time_data.describe()))\n",
    "\n",
    "  # replace original terminal stop records with corrected records\n",
    "  # TODO: explain why only 40k records are dropped instad of 70k\n",
    "\n",
    "  stop_time_data = stop_time_data.drop(terminal_stop_time_data.index)\n",
    "\n",
    "  print('stop_time_data post-drop:\\n{}'.format(stop_time_data.describe()))\n",
    "\n",
    "  # unidentified_stop_time_data = stop_time_data.loc[\n",
    "  #   pd.isnull(stop_time_data.loc[:, 'stop_id'])]\n",
    "  #\n",
    "  # print('unidentified_stop_time_data:\\n{}'.format(unidentified_stop_time_data.describe()))\n",
    "  #\n",
    "  # # TODO: account for runs that begin at a stop other than the terminal\n",
    "  #\n",
    "  # combined_stop_time_data = pd.concat(\n",
    "  #   [terminal_stop_time_data, unidentified_stop_time_data])\n",
    "\n",
    "  # order by index so that we can find contiguous sequences\n",
    "  terminal_stop_time_data.sort_index(inplace=True)\n",
    "\n",
    "  # print('combined_stop_time_data:\\n{}'.format(combined_stop_time_data.describe()))\n",
    "\n",
    "  # construct valid terminal stop records\n",
    "  #TODO: split the compute across a pool of threads, perhaps per time unit\n",
    "  collapsed_terminal_stop_time_data = []\n",
    "\n",
    "  count = 1\n",
    "  seq_len = 1\n",
    "\n",
    "  head_record = terminal_stop_time_data.iloc[0]\n",
    "  head_index = terminal_stop_time_data.index[0]\n",
    "\n",
    "  # ensure head_record is never a BLANK\n",
    "  while head_record.loc['stop_id'].squeeze() == 0 \\\n",
    "      and count < terminal_stop_time_data.shape[0]:\n",
    "    head_record = terminal_stop_time_data.iloc[count]\n",
    "\n",
    "    count += 1\n",
    "\n",
    "  tail_record = head_record\n",
    "  tail_index = head_index\n",
    "\n",
    "  while count < terminal_stop_time_data.shape[0]:\n",
    "    current_record = terminal_stop_time_data.iloc[count]\n",
    "\n",
    "    # TODO: infer stop_ids from records with null stop ids (but skip them for now)\n",
    "    while current_record.loc['stop_id'].squeeze() == 0 \\\n",
    "        and count < terminal_stop_time_data.shape[0] - 1:\n",
    "      seq_len += 1\n",
    "\n",
    "      count += 1\n",
    "\n",
    "      current_record = terminal_stop_time_data.iloc[count]\n",
    "\n",
    "    current_index = terminal_stop_time_data.index[count]\n",
    "\n",
    "    if current_index == head_index + seq_len \\\n",
    "        and current_record.loc['stop_id'].squeeze() == \\\n",
    "        head_record.loc['stop_id'].squeeze():\n",
    "      tail_record = current_record\n",
    "\n",
    "      tail_index = current_index\n",
    "\n",
    "      seq_len += 1\n",
    "    else:\n",
    "      if head_index == tail_index:\n",
    "        # no use in performing unnecessary computation\n",
    "        collapsed_terminal_stop_time_data.append(head_record)\n",
    "      else:\n",
    "        result_record = pd.Series(head_record)\n",
    "\n",
    "        result_record.loc[\n",
    "          ['departed_at', 'departure_latitude', 'departure_longitude']\n",
    "        ] = tail_record.loc[\n",
    "          ['departed_at', 'departure_latitude', 'departure_longitude']]\n",
    "\n",
    "        collapsed_terminal_stop_time_data.append(result_record)\n",
    "\n",
    "      head_record = current_record\n",
    "      head_index = current_index\n",
    "\n",
    "      tail_record = current_record\n",
    "      tail_index = current_index\n",
    "\n",
    "      seq_len = 1\n",
    "\n",
    "    count += 1\n",
    "\n",
    "  print('count: {}'.format(count))\n",
    "\n",
    "  collapsed_terminal_stop_time_data = pd.DataFrame(\n",
    "    collapsed_terminal_stop_time_data)\n",
    "\n",
    "  print('collapsed_terminal_stop_time_data:\\n{}'.format(\n",
    "    collapsed_terminal_stop_time_data.describe()))\n",
    "\n",
    "  # reset indices after removing some records\n",
    "  # stop_time_data.set_index(pd.RangeIndex(stop_time_data.shape[0]), inplace=True)\n",
    "\n",
    "  # print('stop_time_data pre-append:\\n{}'.format(stop_time_data.describe()))\n",
    "\n",
    "  stop_time_data = stop_time_data.append(\n",
    "    collapsed_terminal_stop_time_data, ignore_index=True)\n",
    "\n",
    "  print('stop_time_data post-append:\\n{}'.format(stop_time_data.describe()))\n",
    "\n",
    "  # place the collapsed records just appended into their original positions\n",
    "  stop_time_data.sort_values(\n",
    "    ['route_id', 'vehicle_id', 'arrived_at', 'departed_at'], inplace=True)\n",
    "\n",
    "  # reset indices (even though they will not make their way into the db)\n",
    "  stop_time_data.set_index(pd.RangeIndex(stop_time_data.shape[0]), inplace=True)\n",
    "\n",
    "  return stop_time_data\n",
    "\n",
    "\n",
    "def output_to_excel(data_root_dir, stop_time_data):\n",
    "  # write outpur to Excel for inspection\n",
    "  excel_writer = pd.ExcelWriter(\n",
    "    path.join(data_root_dir, 'processed_stop_times.xlsx'))\n",
    "\n",
    "  stop_time_data.to_excel(excel_writer, 'StopTimes', index=False)\n",
    "\n",
    "  excel_writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--if_exists'], dest='if_exists', nargs=None, const=None, default='append', type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--root_dir', default = r'\\\\vntscex.local\\DFS\\3BC-Share$_Mobileye_Data\\Data\\Data Integration - All Months')\n",
    "parser.add_argument('--db_path', default='ituran_synchromatics_data.sqlite')\n",
    "parser.add_argument('--stop_event_table_name', default='stop_time')\n",
    "parser.add_argument('--root_stop_time_data_dir', default='data_sources')\n",
    "parser.add_argument('--root_route_stop_data_dir', default='route_stops')\n",
    "parser.add_argument('--if_exists', default='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([]) # Empty list to make it work in notbook\n",
    "\n",
    "db_path = 'sqlite:///' + path.join(args.root_dir, args.db_path)\n",
    "db = create_engine(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\vntscex.local\\\\DFS\\\\3BC-Share$_Mobileye_Data\\\\Data\\\\Data Integration - All Months\\\\data_sources'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db\n",
    "path.join(args.root_dir, args.root_stop_time_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!! Not working as a Jupyter Notebook 2019-03-26, now trying in script with modifications to root dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-211b02beb4a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstop_time_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_stop_time_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_stop_time_data_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# read route stops to get terminal stop ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mroute_stop_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_route_stop_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_route_stop_data_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-533b46c8c4de>\u001b[0m in \u001b[0;36mread_stop_time_data\u001b[1;34m(data_root_dir)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     stop_time_data = pd.concat(\n\u001b[1;32m---> 58\u001b[1;33m     stop_time_data, ignore_index=True, verify_integrity=True)\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m   \u001b[1;31m# count the unique stop_tim_id and compare with the number of records to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No objects to concatenate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "stop_time_data = read_stop_time_data(path.join(args.root_dir, args.root_stop_time_data_dir))\n",
    "\n",
    "# read route stops to get terminal stop ids\n",
    "route_stop_data = read_route_stop_data(path.join(args.root_dir, args.root_route_stop_data_dir))\n",
    "\n",
    "# stop_time_data = prune_stop_time_data(stop_time_data, route_stop_data)\n",
    "\n",
    "# poor performance has been observed when adding more than one million records\n",
    "# at a time\n",
    "stop_time_data.to_sql(\n",
    "  args.stop_event_table_name, db, if_exists=args.if_exists, chunksize=1000000,\n",
    "  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
