{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a postgreSQL database -- Jupyter Notebook version\n",
    "\n",
    "*Converted from add_warnigns_with_status_time_open_to_db.py*\n",
    "\n",
    "Create a driver_schedule table, and for each month, and for each route, add all records to that single table\n",
    "\n",
    "We may care to sort the records before adding to the database table\n",
    "\n",
    "First, we need to know if it is safe to use vehicle_assignment_id as the primary key for driver schedule records, so we test for uniqueness across all data files: for each VehiclesThatRanRoute file across all routes and months, read vehicle_assignment_id values into an array, count the unique array entries and compare for equality with the array length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path, listdir\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_warning_name(elem):\n",
    "  warning_name = elem.split(' - StatusTimeOpen:')[0]\n",
    "  return warning_name if warning_name in warning_name_list else None\n",
    "\n",
    "def preprocess_bus_number(elem):\n",
    "  return elem.split()[-1]\n",
    "\n",
    "#assume that the warnings folder only has warning spreadsheet files as children\n",
    "project_root_dir = r'\\\\vntscex.local\\DFS\\3BC-Share$_Mobileye_Data\\Data\\Data Integration' \n",
    "data_root_dir = path.join(project_root_dir, 'warnings') # 'warnings'\n",
    "\n",
    "warning_data = []\n",
    "\n",
    "warning_name_list = [\n",
    "  'ME - Pedestrian Collision Warning', 'ME - Pedestrian In Range Warning',\n",
    "  'PCW-LF', 'PCW-LR', 'PCW-RR', 'PDZ - Left Front', 'PDZ-LR', 'PDZ-R',\n",
    "  'Safety - Braking - Aggressive', 'Safety - Braking - Dangerous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DASH_Report_2018_06_01_to_2018_06_05.xlsx\n",
      "0:00:24.281876\n",
      "DASH_Report_2018_06_06_to_2018_06_10.xlsx\n",
      "0:00:52.018358\n",
      "DASH_Report_2018_06_11_to_2018_06_14.xlsx\n",
      "0:01:24.223020\n",
      "DASH_Report_2018_06_15_to_2018_06_19.xlsx\n",
      "0:01:50.595656\n",
      "DASH_Report_2018_06_20_to_2018_06_24.xlsx\n",
      "0:02:20.681409\n",
      "DASH_Report_2018_06_25_to_2018_06_29.xlsx\n",
      "0:02:57.764290\n",
      "DASH_Report_2018_06_30.xlsx\n",
      "0:02:59.474831\n"
     ]
    }
   ],
   "source": [
    "StartTime = datetime.datetime.now()\n",
    "\n",
    "allfiles = listdir(data_root_dir)\n",
    "# skip one bad file\n",
    "usefiles = (x for x in allfiles if x not in 'DASH_Report_2018_04_16_to_2018_04_20.xlsx')\n",
    "\n",
    "for file_name in usefiles:\n",
    "    \n",
    "    file_path = path.join(data_root_dir, file_name)\n",
    "    print(file_name)\n",
    "    # file_path = path.join(data_root_dir, listdir(data_root_dir)[0])\n",
    "    # print(file_path)\n",
    "\n",
    "    # only read columns loc_time (0), Vehicle Name (2), Address (7),\n",
    "    # warning_name (9), Latitude (11), Longitude (12), and skip the Ituran header\n",
    "    # (first 7 rows)\n",
    "    df = pd.read_excel(file_path,\n",
    "                       skiprows = [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "                       usecols = [0, 1, 3, 4, 5, 6],\n",
    "                       names=['loc_time', 'bus_number', 'address', 'warning_name', 'latitude', 'longitude'],\n",
    "                       header=None, parse_dates=[0])#, dtype={0: object, 1: object, 3: object, 4: object, 5: np.float64, 6: np.float64})\n",
    "\n",
    "    # print(df.describe())\n",
    "    # df.head()\n",
    "    # remove extraneous StatusTimeOpen suffix from warning messages and set other\n",
    "    # messages to null, then drop those null records\n",
    "    df.loc[:, 'warning_name'] = df.loc[:, 'warning_name'].apply(\n",
    "    preprocess_warning_name)\n",
    "\n",
    "    df.loc[:, 'bus_number'] = df.loc[:, 'bus_number'].apply(\n",
    "    preprocess_bus_number).astype(np.uint32)\n",
    "    # print(df.head().loc[:, 'warning_name'])\n",
    "\n",
    "    df.dropna(subset=['warning_name'], inplace=True)\n",
    "    # print(df.describe())\n",
    "    # print(df.head().loc[:, 'warning_name'])\n",
    "\n",
    "   # print(df.head(2))\n",
    "   # print(df.dtypes)\n",
    "    EndTime = datetime.datetime.now()\n",
    "    print(EndTime-StartTime)\n",
    "    warning_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init warning_data:\n",
      "          bus_number       latitude      longitude\n",
      "count  783235.000000  783235.000000  783235.000000\n",
      "mean    14253.054352      34.046502    -118.254150\n",
      "std      2033.392978       0.008839       0.010047\n",
      "min      6303.000000      34.018186    -118.291708\n",
      "25%     12311.000000      34.043236    -118.259266\n",
      "50%     13327.000000      34.047960    -118.254770\n",
      "75%     15341.000000      34.052240    -118.249606\n",
      "max     17312.000000      34.067246    -118.231073\n",
      "de-duplicated warning_data:\n",
      "          bus_number       latitude      longitude\n",
      "count  783232.000000  783232.000000  783232.000000\n",
      "mean    14253.059203      34.046502    -118.254150\n",
      "std      2033.395145       0.008839       0.010047\n",
      "min      6303.000000      34.018186    -118.291708\n",
      "25%     12311.000000      34.043236    -118.259266\n",
      "50%     13327.000000      34.047960    -118.254770\n",
      "75%     15341.000000      34.052240    -118.249606\n",
      "max     17312.000000      34.067246    -118.231073\n",
      "\n",
      "             loc_time  bus_number  \\\n",
      "0 2018-06-01 06:09:01       12301   \n",
      "1 2018-06-01 06:09:10       12301   \n",
      "2 2018-06-01 06:10:38       12301   \n",
      "3 2018-06-01 06:10:40       12301   \n",
      "4 2018-06-01 06:15:40       12301   \n",
      "\n",
      "                                             address  \\\n",
      "0  201 W Washington Blvd, Los Angeles, CA 90015, USA   \n",
      "1  201-221 W Washington Blvd, Los Angeles, CA 900...   \n",
      "2  525-599 W Washington Blvd, Los Angeles, CA 900...   \n",
      "3  525-599 W Washington Blvd, Los Angeles, CA 900...   \n",
      "4  636-644 S Figueroa St, Los Angeles, CA 90017, USA   \n",
      "\n",
      "                       warning_name   latitude   longitude  \n",
      "0  ME - Pedestrian In Range Warning  34.032126 -118.266768  \n",
      "1                             PDZ-R  34.032456 -118.267488  \n",
      "2  ME - Pedestrian In Range Warning  34.034435 -118.271866  \n",
      "3                  PDZ - Left Front  34.034461 -118.271880  \n",
      "4  ME - Pedestrian In Range Warning  34.049881 -118.259318  \n"
     ]
    }
   ],
   "source": [
    "warning_data = pd.concat(\n",
    "  warning_data, ignore_index=True, verify_integrity=True)\n",
    "\n",
    "print('init warning_data:\\n{}'.format(warning_data.describe()))\n",
    "\n",
    "# count the unique stop_tim_id and compare with the number of records to\n",
    "# identify duplicates (and do it per route in case duplicates occur across\n",
    "# routes but not within a single route - which is okay) we learn that indeed\n",
    "# the stop ids are unique within a given route\n",
    "# find_duplicates(warning_data)\n",
    "\n",
    "# drop duplicates if found\n",
    "warning_data.drop_duplicates(inplace=True)\n",
    "\n",
    "print('de-duplicated warning_data:\\n{}'.format(warning_data.describe()))\n",
    "print('\\n{}'.format(warning_data.head()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We temporarily also drop records with missing values to prove our concept.\n",
    "Key attributes that require values include 1) __, 2) route_id,\n",
    "3) vehicle_id, 4) arrived_at, 5) departed_at, and 6) stop_time_id. For now,\n",
    "we exclude the stop_id because many relevant records have missing stop_ids.\n",
    "\n",
    "TODO: Infer missing values where possible using warning and route data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_column_names = ['route_id', 'vehicle_id', 'arrived_at', 'departed_at']\n",
    "#\n",
    "# warning_data.dropna(subset=key_column_names, inplace=True)\n",
    "\n",
    "# we make no assumption about the order in which source files are input\n",
    "warning_data.sort_values(['loc_time', 'bus_number'], inplace=True)\n",
    "\n",
    "# reset indices after sorting records\n",
    "warning_data.set_index(pd.RangeIndex(warning_data.shape[0]), inplace=True)\n",
    "\n",
    "# excel_writer = pd.ExcelWriter('processed_warnings.xlsx')\n",
    "#\n",
    "# chunk_size = pow(2, 20) - 1\n",
    "#\n",
    "# idx_limit = warning_data.shape[0]\n",
    "#\n",
    "# for i in range(int(ceil(idx_limit / chunk_size))):\n",
    "#   chunk = warning_data.iloc[i * chunk_size:max((i + 1) * chunk_size, idx_limit)]\n",
    "#\n",
    "#   print('{}_th chunk_data:\\n{}\\n{}\\n'.format(i, chunk.describe(), chunk.head()))\n",
    "#\n",
    "#   chunk.to_excel(excel_writer, 'warnings_{}'.format(i), index=False)\n",
    "#\n",
    "# excel_writer.save()\n",
    "        \n",
    "#db_path = 'sqlite:///ituran_synchromatics_data_DanTest.sqlite'\n",
    "\n",
    "db_path = 'sqlite:///' + path.join(project_root_dir, 'ituran_synchromatics_data_DanTest.sqlite')\n",
    "db = create_engine(db_path)\n",
    "\n",
    "# poor performance has been observed when adding more than one million records\n",
    "# at a time\n",
    "warning_data.to_sql(\n",
    "  'warning', db, if_exists='replace', chunksize=1000000, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
