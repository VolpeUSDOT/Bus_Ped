{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a driver_schedule table, and\n",
    "for each month, and for each route, add all records to that single table\n",
    "\n",
    "we may care to sort the records before adding to the database table\n",
    "\n",
    "first, we need to know if it is safe to use vehicle_assignment_id as the\n",
    "primary key for driver schedule records, so we test for uniqueness across all\n",
    "data files: for each VehiclesThatRanRoute file across all routes and months,\n",
    "read vehicle_assignment_id values into an array, count the unique array\n",
    "entries and compare for equality with the array length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path, walk\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Search over .zip files in LADOT Synchromatics data\n",
    "project_root_dir = r'\\\\vntscex.local\\DFS\\3BC-Share$_Mobileye_Data\\Data' \n",
    "data_root_dir = path.join(project_root_dir, 'LADOT Sycromatics Data') # was 'data_sources'\n",
    "\n",
    "vehicle_assignment_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Route Data Export 1-1-2018 to 2-1-2018.zip', 'Route Data Export 1-1-2019 to 2-1-2019.zip', 'Route Data Export 10-1-2018 to 11-1-2018.zip', 'Route Data Export 11-1-2018 to 12-1-2018.zip', 'Route Data Export 12-1-2018 to 1-1-2019.zip', 'Route Data Export 2-1-2018 to 3-1-2018.zip', 'Route Data Export 3-1-2018 to 4-1-2018.zip', 'Route Data Export 4-1-2018 to 5-1-2018.zip', 'Route Data Export 5-1-2018 to 6-1-2018.zip', 'Route Data Export 6-1-2018 to 7-1-2018.zip', 'Route Data Export 7-1-2018 to 8-1-2018.zip', 'Route Data Export 8-1-2018 to 9-1-2018.zip', 'Route Data Export 9-1-2018 to 10-1-2018.zip']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dir, subdirs, files in walk(data_root_dir):\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir, subdirs, files in walk(data_root_dir):\n",
    "  # we assume that files only exist at the nodes\n",
    "  if len(files) > 0:\n",
    "    # we assume that only one driver schedule file exists in the current dir\n",
    "    try:\n",
    "      file_name_indices = [\n",
    "        file.find('_VehiclesThatRanRoute_') >= 0 for file in files]\n",
    "\n",
    "      file_name_index = file_name_indices.index(True)\n",
    "      file_name = files[file_name_index]\n",
    "      file_path = path.join(dir, file_name)\n",
    "\n",
    "      # forget using np.unicode_ for strings since pandas treats them as objects\n",
    "      # we can specify the data type since none of the values are null\n",
    "      df = pd.read_table(\n",
    "        file_path, usecols=[0, 1, 2, 3, 5, 6, 11, 12, 13, 14],\n",
    "        header=None, skiprows=[0], parse_dates=['start_time', 'end_time'],\n",
    "        names=['vehicle_assignment_id', 'vehicle_id', 'route_id', 'driver_id',\n",
    "               'start_time', 'end_time', 'bus_number', 'first_name',\n",
    "               'last_name', 'badge_number'],\n",
    "        dtype={'vehicle_assignment_id': np.uint64, 'vehicle_id': np.uint32,\n",
    "               'route_id': np.uint32, 'driver_id': np.uint32,\n",
    "               'start_time': object, 'end_time': object,\n",
    "               'bus_number': np.uint32, 'first_name': object,\n",
    "               'last_name': object, 'badge_number': np.uint32})\n",
    "\n",
    "      print(df.head(2))\n",
    "      print(df.dtypes)\n",
    "\n",
    "      vehicle_assignment_data.append(df)\n",
    "    except Exception as e:\n",
    "      print('Driver schedule file not found in {}'.format(dir))\n",
    "      print(e)\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_assignment_data = pd.concat(\n",
    "  vehicle_assignment_data, ignore_index=True, verify_integrity=True)\n",
    "\n",
    "# records of runs that span two days may appear once for each day depending on\n",
    "# how the Excel exports were preformed, and should be dropped\n",
    "vehicle_assignment_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# we temporarily also drop records with missing values to prove our concept.\n",
    "# Key attributes that require values include 1) vehicle_assessment_id,\n",
    "# 2) vehicle_id, 3) BusNumber, 4) driver_id (at least for longitudinal),\n",
    "# 5) start_time, and 6) end_time.\n",
    "# TODO: Infer missing values where possible using warning and route data\n",
    "key_column_names = ['vehicle_assignment_id', 'vehicle_id', 'bus_number',\n",
    "                    'driver_id', 'start_time', 'end_time']\n",
    "\n",
    "vehicle_assignment_data.dropna(subset=key_column_names, inplace=True)\n",
    "\n",
    "# we make no assumption about the order in which source xlsx files are input\n",
    "vehicle_assignment_data.sort_values(['start_time', 'end_time'], inplace=True)\n",
    "\n",
    "# after removing duplicate records, vehicle_assignment_ids will be unique and\n",
    "# can be used as the primary key of the vehicle_assignment table. Because we\n",
    "# don't yet know how this results in a SQLite PK, just reset the indices for now\n",
    "# vehicle_assignment_data.set_index('vehicle_assignment_id', inplace=True)\n",
    "vehicle_assignment_data.set_index(\n",
    "  pd.RangeIndex(vehicle_assignment_data.shape[0]), inplace=True)\n",
    "\n",
    "print(vehicle_assignment_data.describe())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'sqlite:///ituran_synchromatics_data.sqlite'\n",
    "\n",
    "db = create_engine(db_path)\n",
    "\n",
    "# poor performance has been observed when adding more than one million records\n",
    "# at a time\n",
    "vehicle_assignment_data.to_sql(\n",
    "  'vehicle_assignment', db, if_exists='replace', chunksize=1000000, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
