{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add stop times\n",
    "\n",
    "Follows Step 3, `03_add_route_stops_to_db.ipynb`\n",
    "\n",
    "Followed by Step 5, `05_generate_data_product_from_db.ipynb`\n",
    "\n",
    "**In process for `ituran_synchromatics_data.sqlite` in Data Integration - All Months**\n",
    "\n",
    "This script creates or replaces a table in the database at the supplied\n",
    "path that contains the set of stops for each of five Downtown DASH routes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path, walk\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from add_route_stops_to_db import read_route_stop_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path='ituran_synchromatics_data.sqlite'\n",
    "stop_event_table_name='stop_time'\n",
    "\n",
    "project_root_dir = r'\\\\vntscex.local\\DFS\\3BC-Share$_Mobileye_Data\\Data\\Data Integration - All Months' \n",
    "\n",
    "root_stop_time_data_dir=path.join(project_root_dir, 'data_sources')\n",
    "root_route_stop_data_dir=path.join(project_root_dir,'route_stops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(df, index_col='stop_time_id', duplicate_col='route_id'):\n",
    "  unique_route_ids = df.loc[:, duplicate_col].unique()\n",
    "\n",
    "  for unique_route_id in unique_route_ids:\n",
    "    routes = df.loc[df[duplicate_col] == unique_route_id]\n",
    "\n",
    "    # display unique record count and total record count for comparison\n",
    "    print(routes.shape[0])\n",
    "    print(routes.loc[:, index_col].unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bus_number(elem):\n",
    "  return elem.split()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert print statements to log statements\n",
    "def read_stop_time_data(data_root_dir):\n",
    "  stop_time_data = []\n",
    "\n",
    "  for dir, subdirs, files in walk(data_root_dir):\n",
    "    # we assume that files only exist at the nodes\n",
    "    if len(files) > 0:\n",
    "      # we assume that only one driver schedule file exists in the current dir\n",
    "      file_name_indices = [\n",
    "        file.find('_StopTimes_') >= 0 for file in files]\n",
    "\n",
    "      try:\n",
    "        file_name_index = file_name_indices.index(True)\n",
    "\n",
    "        file_name = files[file_name_index]\n",
    "\n",
    "        file_path = path.join(dir, file_name)\n",
    "\n",
    "        df = pd.read_csv(\n",
    "          file_path, sep='\\t', usecols=[0, 1, 2, 4, 5, 6, 7, 8, 9, 12],\n",
    "          parse_dates=['arrived_at', 'departed_at'], dtype={\n",
    "            'stop_id': object, 'route_id': np.uint32, 'vehicle_id': np.uint16,\n",
    "            'arrived_at': object, 'arrival_latitude': np.float64,\n",
    "            'arrival_longitude': np.float64, 'departed_at': object,\n",
    "            'departure_latitude': np.float64, 'departure_longitude': np.float64,\n",
    "            'stop_time_id': np.uint64})\n",
    "\n",
    "        # convert null stop_ids to a zero value\n",
    "        df['stop_id'] = np.array(\n",
    "          df['stop_id'].values, dtype=np.float32).astype(np.uint32)\n",
    "\n",
    "        # print(df.head(2))\n",
    "        # print(df.dtypes)\n",
    "\n",
    "        stop_time_data.append(df)\n",
    "      #TODO: discover and handle distinct exceptions rather than catch all\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "  stop_time_data = pd.concat(\n",
    "    stop_time_data, ignore_index=True, verify_integrity=True)\n",
    "\n",
    "  # count the unique stop_tim_id and compare with the number of records to\n",
    "  # identify duplicates (and do it per route in case duplicates occur across\n",
    "  # routes but not within a single route - which is okay) we learn that indeed\n",
    "  # the stop ids are unique within a given route\n",
    "  # ...we don't call this anymore having observed that no duplicates exist (for now)\n",
    "  # find_duplicates(stop_time_data)\n",
    "\n",
    "  # drop duplicates if found\n",
    "  stop_time_data.drop_duplicates(inplace=True)\n",
    "\n",
    "  # we temporarily also drop records with missing values to prove our concept.\n",
    "  # Key attributes that require values include 1) __, 2) route_id,\n",
    "  # 3) vehicle_id, 4) arrived_at, 5) departed_at, and 6) stop_time_id. For now,\n",
    "  # we exclude the stop_id because many relevant records have missing stop_ids.\n",
    "  # TODO: Infer missing values where possible using warning and route data\n",
    "  key_column_names = ['route_id', 'vehicle_id', 'arrived_at', 'departed_at']\n",
    "\n",
    "  stop_time_data.dropna(subset=key_column_names, inplace=True)\n",
    "\n",
    "  stop_time_data.drop(stop_time_data.query('stop_id == 0').index, inplace=True)\n",
    "\n",
    "  # we make no assumption about the order in which source files are input\n",
    "  stop_time_data.sort_values(\n",
    "    ['route_id', 'vehicle_id', 'arrived_at', 'departed_at'], inplace=True)\n",
    "\n",
    "  # reset indices after removing some records\n",
    "  stop_time_data.set_index(pd.RangeIndex(stop_time_data.shape[0]), inplace=True)\n",
    "  print(stop_time_data.describe())\n",
    "  return stop_time_data\n",
    "# we must identify terminal stop records and collapse sequences of records of a\n",
    "# single terminal into a single record. We extract the set of terminal stops\n",
    "# from the 'route_stop' table in the existing database\n",
    "\n",
    "# get terminal stops from Excel in case a route stop table has not yet been\n",
    "# created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_stop_time_data(stop_time_data, route_stop_data):\n",
    "  terminal_stop_data = route_stop_data.loc[\n",
    "    route_stop_data.loc[:, 'sequence'] == 1]\n",
    "\n",
    "  terminal_stop_time_data = []\n",
    "\n",
    "  # TODO handle discontinuity at 12AM.\n",
    "  # do any records have timestamps between 2130 and 0030?\n",
    "  for stop_id in terminal_stop_data['stop_id']:\n",
    "    terminal_stop_time_data.append(\n",
    "      stop_time_data[stop_time_data['stop_id'] == stop_id])\n",
    "\n",
    "  terminal_stop_time_data = pd.concat(terminal_stop_time_data)\n",
    "\n",
    "  print('terminal_stop_time_data:\\n{}'.format(terminal_stop_time_data.describe()))\n",
    "\n",
    "  print('stop_time_data pre-drop:\\n{}'.format(stop_time_data.describe()))\n",
    "\n",
    "  # replace original terminal stop records with corrected records\n",
    "  # TODO: explain why only 40k records are dropped instad of 70k\n",
    "\n",
    "  stop_time_data = stop_time_data.drop(terminal_stop_time_data.index)\n",
    "\n",
    "  print('stop_time_data post-drop:\\n{}'.format(stop_time_data.describe()))\n",
    "\n",
    "  # unidentified_stop_time_data = stop_time_data.loc[\n",
    "  #   pd.isnull(stop_time_data.loc[:, 'stop_id'])]\n",
    "  #\n",
    "  # print('unidentified_stop_time_data:\\n{}'.format(unidentified_stop_time_data.describe()))\n",
    "  #\n",
    "  # # TODO: account for runs that begin at a stop other than the terminal\n",
    "  #\n",
    "  # combined_stop_time_data = pd.concat(\n",
    "  #   [terminal_stop_time_data, unidentified_stop_time_data])\n",
    "\n",
    "  # order by index so that we can find contiguous sequences\n",
    "  terminal_stop_time_data.sort_index(inplace=True)\n",
    "\n",
    "  # print('combined_stop_time_data:\\n{}'.format(combined_stop_time_data.describe()))\n",
    "\n",
    "  # construct valid terminal stop records\n",
    "  #TODO: split the compute across a pool of threads, perhaps per time unit\n",
    "  collapsed_terminal_stop_time_data = []\n",
    "\n",
    "  count = 1\n",
    "  seq_len = 1\n",
    "\n",
    "  head_record = terminal_stop_time_data.iloc[0]\n",
    "  head_index = terminal_stop_time_data.index[0]\n",
    "\n",
    "  # ensure head_record is never a BLANK\n",
    "  while head_record.loc['stop_id'].squeeze() == 0 \\\n",
    "      and count < terminal_stop_time_data.shape[0]:\n",
    "    head_record = terminal_stop_time_data.iloc[count]\n",
    "\n",
    "    count += 1\n",
    "\n",
    "  tail_record = head_record\n",
    "  tail_index = head_index\n",
    "\n",
    "  while count < terminal_stop_time_data.shape[0]:\n",
    "    current_record = terminal_stop_time_data.iloc[count]\n",
    "\n",
    "    # TODO: infer stop_ids from records with null stop ids (but skip them for now)\n",
    "    while current_record.loc['stop_id'].squeeze() == 0 \\\n",
    "        and count < terminal_stop_time_data.shape[0] - 1:\n",
    "      seq_len += 1\n",
    "\n",
    "      count += 1\n",
    "\n",
    "      current_record = terminal_stop_time_data.iloc[count]\n",
    "\n",
    "    current_index = terminal_stop_time_data.index[count]\n",
    "\n",
    "    if current_index == head_index + seq_len \\\n",
    "        and current_record.loc['stop_id'].squeeze() == \\\n",
    "        head_record.loc['stop_id'].squeeze():\n",
    "      tail_record = current_record\n",
    "\n",
    "      tail_index = current_index\n",
    "\n",
    "      seq_len += 1\n",
    "    else:\n",
    "      if head_index == tail_index:\n",
    "        # no use in performing unnecessary computation\n",
    "        collapsed_terminal_stop_time_data.append(head_record)\n",
    "      else:\n",
    "        result_record = pd.Series(head_record)\n",
    "\n",
    "        result_record.loc[\n",
    "          ['departed_at', 'departure_latitude', 'departure_longitude']\n",
    "        ] = tail_record.loc[\n",
    "          ['departed_at', 'departure_latitude', 'departure_longitude']]\n",
    "\n",
    "        collapsed_terminal_stop_time_data.append(result_record)\n",
    "\n",
    "      head_record = current_record\n",
    "      head_index = current_index\n",
    "\n",
    "      tail_record = current_record\n",
    "      tail_index = current_index\n",
    "\n",
    "      seq_len = 1\n",
    "\n",
    "    count += 1\n",
    "\n",
    "  print('count: {}'.format(count))\n",
    "\n",
    "  collapsed_terminal_stop_time_data = pd.DataFrame(\n",
    "    collapsed_terminal_stop_time_data)\n",
    "\n",
    "  print('collapsed_terminal_stop_time_data:\\n{}'.format(\n",
    "    collapsed_terminal_stop_time_data.describe()))\n",
    "\n",
    "  # reset indices after removing some records\n",
    "  # stop_time_data.set_index(pd.RangeIndex(stop_time_data.shape[0]), inplace=True)\n",
    "\n",
    "  # print('stop_time_data pre-append:\\n{}'.format(stop_time_data.describe()))\n",
    "\n",
    "  stop_time_data = stop_time_data.append(\n",
    "    collapsed_terminal_stop_time_data, ignore_index=True)\n",
    "\n",
    "  print('stop_time_data post-append:\\n{}'.format(stop_time_data.describe()))\n",
    "\n",
    "  # place the collapsed records just appended into their original positions\n",
    "  stop_time_data.sort_values(\n",
    "    ['route_id', 'vehicle_id', 'arrived_at', 'departed_at'], inplace=True)\n",
    "\n",
    "  # reset indices (even though they will not make their way into the db)\n",
    "  stop_time_data.set_index(pd.RangeIndex(stop_time_data.shape[0]), inplace=True)\n",
    "\n",
    "  return stop_time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_excel(data_root_dir, stop_time_data):\n",
    "  # write outpur to Excel for inspection\n",
    "  excel_writer = pd.ExcelWriter(\n",
    "    path.join(data_root_dir, 'processed_stop_times.xlsx'))\n",
    "\n",
    "  stop_time_data.to_excel(excel_writer, 'StopTimes', index=False)\n",
    "\n",
    "  excel_writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "            stop_id      route_id    vehicle_id  arrival_latitude  \\\n",
      "count  5.769190e+06  5.769190e+06  5.769190e+06      5.769190e+06   \n",
      "mean   1.459268e+05  2.652156e+03  1.428878e+03      3.404596e+01   \n",
      "std    4.192426e+05  3.902615e+03  1.082743e+03      4.805043e-02   \n",
      "min    4.219900e+04  2.960000e+02  3.030000e+02      0.000000e+00   \n",
      "25%    9.045300e+04  2.970000e+02  3.630000e+02      3.404073e+01   \n",
      "50%    9.095900e+04  2.980000e+02  1.574000e+03      3.404838e+01   \n",
      "75%    9.142100e+04  8.435000e+03  1.630000e+03      3.405289e+01   \n",
      "max    3.986127e+06  9.960000e+03  4.386000e+03      3.410552e+01   \n",
      "\n",
      "       arrival_longitude  departure_latitude  departure_longitude  \\\n",
      "count       5.769190e+06        5.769190e+06         5.769190e+06   \n",
      "mean       -1.182533e+02        3.404604e+01        -1.182535e+02   \n",
      "std         1.779456e-01        1.731602e-02         5.063240e-02   \n",
      "min        -1.183388e+02        0.000000e+00        -1.183388e+02   \n",
      "25%        -1.182595e+02        3.404051e+01        -1.182592e+02   \n",
      "50%        -1.182542e+02        3.404838e+01        -1.182544e+02   \n",
      "75%        -1.182447e+02        3.405297e+01        -1.182449e+02   \n",
      "max         1.833333e-01        3.410550e+01         0.000000e+00   \n",
      "\n",
      "       stop_time_id  \n",
      "count  5.769190e+06  \n",
      "mean   1.452280e+07  \n",
      "std    9.875898e+06  \n",
      "min    6.300000e+01  \n",
      "25%    4.317787e+06  \n",
      "50%    1.360272e+07  \n",
      "75%    2.272999e+07  \n",
      "max    3.413631e+07  \n"
     ]
    }
   ],
   "source": [
    "db_path = 'sqlite:///' + path.join(project_root_dir, 'ituran_synchromatics_data.sqlite')\n",
    "\n",
    "# db_path = 'sqlite:///' + args.db_path\n",
    "\n",
    "db = create_engine(db_path)\n",
    "\n",
    "stop_time_data = read_stop_time_data(root_stop_time_data_dir)\n",
    "\n",
    "# read route stops to get terminal stop ids\n",
    "route_stop_data = read_route_stop_data(root_route_stop_data_dir)\n",
    "\n",
    "# stop_time_data = prune_stop_time_data(stop_time_data, route_stop_data)\n",
    "\n",
    "# poor performance has been observed when adding more than one million records\n",
    "# at a time\n",
    "stop_time_data.to_sql(\n",
    "    stop_event_table_name, db, if_exists='append', chunksize=1000000,\n",
    "   index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
