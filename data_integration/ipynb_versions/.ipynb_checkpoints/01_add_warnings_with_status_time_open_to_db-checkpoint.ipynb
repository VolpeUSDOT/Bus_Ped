{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Create a postgreSQL database -- Jupyter Notebook version\n",
    "\n",
    "**Successfully run on complete historical data, creating `ituran_synchromatics_data.sqlite` in Data Integration - All Months**\n",
    "\n",
    "*Converted from add_warnings_with_status_time_open_to_db.py*\n",
    "\n",
    "Create a driver_schedule table, and for each month, and for each route, add all records to that single table\n",
    "\n",
    "We may care to sort the records before adding to the database table\n",
    "\n",
    "First, we need to know if it is safe to use vehicle_assignment_id as the primary key for driver schedule records, so we test for uniqueness across all data files: for each VehiclesThatRanRoute file across all routes and months, read vehicle_assignment_id values into an array, count the unique array entries and compare for equality with the array length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path, listdir\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_warning_name(elem):\n",
    "  warning_name = elem.split(' - StatusTimeOpen:')[0]\n",
    "  return warning_name if warning_name in warning_name_list else None\n",
    "\n",
    "def preprocess_bus_number(elem):\n",
    "  return elem.split()[-1]\n",
    "\n",
    "#assume that the warnings folder only has warning spreadsheet files as children\n",
    "project_root_dir = r'\\\\vntscex.local\\DFS\\3BC-Share$_Mobileye_Data\\Data\\Data Integration - All Months' \n",
    "data_root_dir = path.join(project_root_dir, 'warnings') # 'warnings'\n",
    "\n",
    "warning_data = []\n",
    "\n",
    "warning_name_list = [\n",
    "  'ME - Pedestrian Collision Warning', 'ME - Pedestrian In Range Warning',\n",
    "  'PCW-LF', 'PCW-LR', 'PCW-RR', 'PDZ - Left Front', 'PDZ-LR', 'PDZ-R',\n",
    "  'Safety - Braking - Aggressive', 'Safety - Braking - Dangerous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DASH_Report_2018_01_01_to_2018_01_31.xlsx\n",
      "0:00:01.445133\n",
      "DASH_Report_2018_02_01_to_2018_02_05.xlsx\n",
      "0:00:08.312629\n",
      "DASH_Report_2018_02_06_to_2018_02_10.xlsx\n",
      "0:00:32.018252\n",
      "DASH_Report_2018_02_10_to_2018_02_15.xlsx\n",
      "0:00:56.879060\n",
      "DASH_Report_2018_02_16_to_2018_02_20.xlsx\n",
      "0:01:26.295113\n",
      "DASH_Report_2018_02_21_to_2018_02_25.xlsx\n",
      "0:02:07.471013\n",
      "DASH_Report_2018_02_26_to_2018_02_28.xlsx\n",
      "0:02:37.760097\n",
      "DASH_Report_2018_03_01_to_2018_03_05.xlsx\n",
      "0:03:33.484092\n",
      "DASH_Report_2018_03_06_to_2018_03_10.xlsx\n",
      "0:04:57.460967\n",
      "DASH_Report_2018_03_11_to_2018_03_15.xlsx\n",
      "0:06:17.745573\n",
      "DASH_Report_2018_03_16_to_2018_03_20.xlsx\n",
      "0:07:27.663547\n",
      "DASH_Report_2018_03_21_to_2018_03_25.xlsx\n",
      "0:08:26.098643\n",
      "DASH_Report_2018_03_26_to_2018_03_30.xlsx\n",
      "0:10:11.752076\n",
      "DASH_Report_2018_03_31.xlsx\n",
      "0:10:16.734438\n",
      "DASH_Report_2018_04_01_to_2018_04_05.xlsx\n",
      "0:11:48.517896\n",
      "DASH_Report_2018_04_06_to_2018_04_10.xlsx\n",
      "0:12:59.849912\n",
      "DASH_Report_2018_04_11_to_2018_04_15.xlsx\n",
      "0:14:11.979963\n",
      "DASH_Report_2018_04_16_to_2018_04_18.xlsx\n",
      "0:15:37.653955\n",
      "DASH_Report_2018_04_19_to_2018_04_20.xlsx\n",
      "0:16:21.140016\n",
      "DASH_Report_2018_04_21_to_2018_04_25.xlsx\n",
      "0:17:35.312288\n",
      "DASH_Report_2018_04_26_to_2018_04_30.xlsx\n",
      "0:18:48.002345\n",
      "DASH_Report_2018_05_01_to_2018_05_05.xlsx\n",
      "0:20:19.858806\n",
      "DASH_Report_2018_05_06_to_2018_05_10.xlsx\n",
      "0:21:52.163300\n",
      "DASH_Report_2018_05_11_to_2018_05_15.xlsx\n",
      "0:23:08.680684\n",
      "DASH_Report_2018_05_16_to_2018_05_20.xlsx\n",
      "0:24:22.966965\n",
      "DASH_Report_2018_05_21_to_2018_05_25.xlsx\n",
      "0:26:10.697484\n",
      "DASH_Report_2018_05_26_to_2018_05_31.xlsx\n",
      "0:27:25.596761\n",
      "DASH_Report_2018_06_01_to_2018_06_05.xlsx\n",
      "0:28:42.756180\n",
      "DASH_Report_2018_06_06_to_2018_06_10.xlsx\n",
      "0:30:00.911681\n",
      "DASH_Report_2018_06_11_to_2018_06_14.xlsx\n",
      "0:31:35.067302\n",
      "DASH_Report_2018_06_15_to_2018_06_19.xlsx\n",
      "0:32:51.677734\n",
      "DASH_Report_2018_06_20_to_2018_06_24.xlsx\n",
      "0:34:07.053991\n",
      "DASH_Report_2018_06_25_to_2018_06_29.xlsx\n",
      "0:36:00.134007\n",
      "DASH_Report_2018_06_30.xlsx\n",
      "0:36:05.400357\n",
      "DASH_Report_2018_07_01_to_2018_07_05.xlsx\n",
      "0:37:07.163663\n",
      "DASH_Report_2018_07_06_to_2018_07_10.xlsx\n",
      "0:38:11.842228\n",
      "DASH_Report_2018_07_11_to_2018_07_15.xlsx\n",
      "0:39:17.492886\n",
      "DASH_Report_2018_07_16_to_2018_07_20 (1).xlsx\n",
      "0:41:01.266128\n",
      "DASH_Report_2018_07_16_to_2018_07_20.xlsx\n",
      "0:42:39.103054\n",
      "DASH_Report_2018_07_21_to_2018_07_25.xlsx\n",
      "0:43:41.103385\n",
      "DASH_Report_2018_07_26_to_2018_07_31.xlsx\n",
      "0:45:03.254209\n",
      "DASH_Report_2018_08_01_to_2018_08_05.xlsx\n",
      "0:46:08.637767\n",
      "DASH_Report_2018_08_06_to_2018_08_10.xlsx\n",
      "0:47:50.741990\n",
      "DASH_Report_2018_08_11_to_2018_08_15.xlsx\n",
      "0:48:58.424751\n",
      "DASH_Report_2018_08_16_to_2018_08_20.xlsx\n",
      "0:50:08.697638\n",
      "DASH_Report_2018_08_21_to_2018_08_25.xlsx\n",
      "0:51:31.463457\n",
      "DASH_Report_2018_08_26_to_2018_08_30.xlsx\n",
      "0:52:56.225480\n",
      "DASH_Report_2018_08_31.xlsx\n",
      "0:53:16.304885\n",
      "DASH_Report_2018_09_01_to_2018_09_05.xlsx\n",
      "0:54:07.780493\n",
      "DASH_Report_2018_09_06_to_2018_09_10.xlsx\n",
      "0:55:15.062189\n",
      "DASH_Report_2018_09_11_to_2018_09_15.xlsx\n",
      "0:56:42.811373\n",
      "DASH_Report_2018_09_16_to_2018_09_20.xlsx\n",
      "0:58:04.889131\n",
      "DASH_Report_2018_09_21_to_2018_09_25.xlsx\n",
      "0:59:11.517817\n",
      "DASH_Report_2018_09_26_to_2018_09_30.xlsx\n",
      "1:00:16.629430\n",
      "DASH_Report_2018_10_01_to_2018_10_04.xlsx\n",
      "1:01:38.902230\n",
      "DASH_Report_2018_10_05_to_2018_10_10.xlsx\n",
      "1:02:55.765593\n",
      "DASH_Report_2018_10_11_to_2018_10_15.xlsx\n",
      "1:03:54.731792\n",
      "DASH_Report_2018_10_16_to_2018_20_15.xlsx\n",
      "1:05:12.051178\n",
      "DASH_Report_2018_10_21_to_2018_10_25.xlsx\n",
      "1:06:20.388041\n",
      "DASH_Report_2018_10_26_to_2018_10_31.xlsx\n",
      "1:07:40.231657\n",
      "DASH_Report_2018_11_01_to_2018_11_05.xlsx\n",
      "1:08:44.163121\n",
      "DASH_Report_2018_11_06_to_2018_11_10.xlsx\n",
      "1:10:03.916754\n",
      "DASH_Report_2018_11_11_to_2018_11_15.xlsx\n",
      "1:11:09.662377\n",
      "DASH_Report_2018_11_16_to_2018_11_20.xlsx\n",
      "1:12:08.639539\n",
      "DASH_Report_2018_11_21_to_2018_11_25.xlsx\n",
      "1:12:42.022894\n",
      "DASH_Report_2018_11_26_to_2018_11_30.xlsx\n",
      "1:14:03.103538\n",
      "DASH_Report_2018_12_01_to_2018_12_05.xlsx\n",
      "1:14:55.463219\n",
      "DASH_Report_2018_12_06_to_2018_12_10.xlsx\n",
      "1:15:44.959757\n",
      "DASH_Report_2018_12_11_to_2018_12_15.xlsx\n",
      "1:16:56.895813\n",
      "DASH_Report_2018_12_16_to_2018_12_20.xlsx\n",
      "1:18:06.082669\n",
      "DASH_Report_2018_12_21_to_2018_12_25.xlsx\n",
      "1:18:41.207149\n",
      "DASH_Report_2018_12_26_to_2018_12_31.xlsx\n",
      "1:19:44.760566\n"
     ]
    }
   ],
   "source": [
    "StartTime = datetime.datetime.now()\n",
    "\n",
    "allfiles = listdir(data_root_dir)\n",
    "# skip one bad file\n",
    "usefiles = (x for x in allfiles if x not in 'DASH_Report_2018_04_16_to_2018_04_20.xlsx')\n",
    "\n",
    "for file_name in usefiles:\n",
    "    \n",
    "    file_path = path.join(data_root_dir, file_name)\n",
    "    print(file_name)\n",
    "    # file_path = path.join(data_root_dir, listdir(data_root_dir)[0])\n",
    "    # print(file_path)\n",
    "\n",
    "    # only read columns loc_time (0), Vehicle Name (2), Address (7),\n",
    "    # warning_name (9), Latitude (11), Longitude (12), and skip the Ituran header\n",
    "    # (first 7 rows)\n",
    "    df = pd.read_excel(file_path,\n",
    "                       skiprows = [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "                       usecols = [0, 1, 3, 4, 5, 6],\n",
    "                       names=['loc_time', 'bus_number', 'address', 'warning_name', 'latitude', 'longitude'],\n",
    "                       header=None, parse_dates=[0])#, dtype={0: object, 1: object, 3: object, 4: object, 5: np.float64, 6: np.float64})\n",
    "\n",
    "    # print(df.describe())\n",
    "    # df.head()\n",
    "    # remove extraneous StatusTimeOpen suffix from warning messages and set other\n",
    "    # messages to null, then drop those null records\n",
    "    df.loc[:, 'warning_name'] = df.loc[:, 'warning_name'].apply(\n",
    "    preprocess_warning_name)\n",
    "\n",
    "    df.loc[:, 'bus_number'] = df.loc[:, 'bus_number'].apply(\n",
    "    preprocess_bus_number).astype(np.uint32)\n",
    "    # print(df.head().loc[:, 'warning_name'])\n",
    "\n",
    "    df.dropna(subset=['warning_name'], inplace=True)\n",
    "    # print(df.describe())\n",
    "    # print(df.head().loc[:, 'warning_name'])\n",
    "\n",
    "   # print(df.head(2))\n",
    "   # print(df.dtypes)\n",
    "    EndTime = datetime.datetime.now()\n",
    "    print(EndTime-StartTime)\n",
    "    warning_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init warning_data:\n",
      "         bus_number      latitude     longitude\n",
      "count  7.510357e+06  7.510357e+06  7.510357e+06\n",
      "mean   1.415943e+04  3.404640e+01 -1.182543e+02\n",
      "std    1.968558e+03  9.101616e-03  1.033914e-02\n",
      "min    6.303000e+03  3.401815e+01 -1.182917e+02\n",
      "25%    1.231000e+04  3.404305e+01 -1.182593e+02\n",
      "50%    1.332700e+04  3.404793e+01 -1.182548e+02\n",
      "75%    1.534000e+04  3.405227e+01 -1.182496e+02\n",
      "max    1.731200e+04  3.406728e+01 -1.182310e+02\n",
      "de-duplicated warning_data:\n",
      "         bus_number      latitude     longitude\n",
      "count  7.272571e+06  7.272571e+06  7.272571e+06\n",
      "mean   1.414969e+04  3.404639e+01 -1.182543e+02\n",
      "std    1.967238e+03  9.098459e-03  1.032162e-02\n",
      "min    6.303000e+03  3.401815e+01 -1.182917e+02\n",
      "25%    1.231000e+04  3.404302e+01 -1.182593e+02\n",
      "50%    1.332700e+04  3.404792e+01 -1.182548e+02\n",
      "75%    1.533900e+04  3.405227e+01 -1.182496e+02\n",
      "max    1.731200e+04  3.406728e+01 -1.182310e+02\n",
      "\n",
      "             loc_time  bus_number  \\\n",
      "0 2018-01-31 15:06:03       15301   \n",
      "1 2018-01-29 13:54:07       15314   \n",
      "2 2018-01-29 14:01:06       15314   \n",
      "3 2018-01-29 14:01:08       15314   \n",
      "4 2018-01-29 14:01:13       15314   \n",
      "\n",
      "                                            address warning_name   latitude  \\\n",
      "0    1001 S Figueroa St, Los Angeles, CA 90015, USA        PDZ-R  34.044915   \n",
      "1  1600-1642 S Olive St, Los Angeles, CA 90015, USA        PDZ-R  34.034995   \n",
      "2     800-802 S Main St, Los Angeles, CA 90014, USA        PDZ-R  34.042676   \n",
      "3     800-802 S Main St, Los Angeles, CA 90014, USA        PDZ-R  34.042676   \n",
      "4     800-802 S Main St, Los Angeles, CA 90014, USA        PDZ-R  34.042676   \n",
      "\n",
      "    longitude  \n",
      "0 -118.264175  \n",
      "1 -118.265511  \n",
      "2 -118.253306  \n",
      "3 -118.253306  \n",
      "4 -118.253306  \n"
     ]
    }
   ],
   "source": [
    "warning_data = pd.concat(\n",
    "  warning_data, ignore_index=True, verify_integrity=True)\n",
    "\n",
    "print('init warning_data:\\n{}'.format(warning_data.describe()))\n",
    "\n",
    "# count the unique stop_tim_id and compare with the number of records to\n",
    "# identify duplicates (and do it per route in case duplicates occur across\n",
    "# routes but not within a single route - which is okay) we learn that indeed\n",
    "# the stop ids are unique within a given route\n",
    "# find_duplicates(warning_data)\n",
    "\n",
    "# drop duplicates if found\n",
    "warning_data.drop_duplicates(inplace=True)\n",
    "\n",
    "print('de-duplicated warning_data:\\n{}'.format(warning_data.describe()))\n",
    "print('\\n{}'.format(warning_data.head()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We temporarily also drop records with missing values to prove our concept.\n",
    "Key attributes that require values include 1) __, 2) route_id,\n",
    "3) vehicle_id, 4) arrived_at, 5) departed_at, and 6) stop_time_id. For now,\n",
    "we exclude the stop_id because many relevant records have missing stop_ids.\n",
    "\n",
    "TODO: Infer missing values where possible using warning and route data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_column_names = ['route_id', 'vehicle_id', 'arrived_at', 'departed_at']\n",
    "#\n",
    "# warning_data.dropna(subset=key_column_names, inplace=True)\n",
    "\n",
    "# we make no assumption about the order in which source files are input\n",
    "warning_data.sort_values(['loc_time', 'bus_number'], inplace=True)\n",
    "\n",
    "# reset indices after sorting records\n",
    "warning_data.set_index(pd.RangeIndex(warning_data.shape[0]), inplace=True)\n",
    "\n",
    "# excel_writer = pd.ExcelWriter('processed_warnings.xlsx')\n",
    "#\n",
    "# chunk_size = pow(2, 20) - 1\n",
    "#\n",
    "# idx_limit = warning_data.shape[0]\n",
    "#\n",
    "# for i in range(int(ceil(idx_limit / chunk_size))):\n",
    "#   chunk = warning_data.iloc[i * chunk_size:max((i + 1) * chunk_size, idx_limit)]\n",
    "#\n",
    "#   print('{}_th chunk_data:\\n{}\\n{}\\n'.format(i, chunk.describe(), chunk.head()))\n",
    "#\n",
    "#   chunk.to_excel(excel_writer, 'warnings_{}'.format(i), index=False)\n",
    "#\n",
    "# excel_writer.save()\n",
    "        \n",
    "#db_path = 'sqlite:///ituran_synchromatics_data_DanTest.sqlite'\n",
    "\n",
    "db_path = 'sqlite:///' + path.join(project_root_dir, 'ituran_synchromatics_data.sqlite')\n",
    "db = create_engine(db_path)\n",
    "\n",
    "# poor performance has been observed when adding more than one million records\n",
    "# at a time\n",
    "warning_data.to_sql(\n",
    "  'warning', db, if_exists='replace', chunksize=1000000, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
